{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "import cPickle as pickle\n",
    "\n",
    "#%pylab inline\n",
    "import lasagne\n",
    "import os\n",
    "import time\n",
    "\n",
    "#import numpy as np\n",
    "#import theano\n",
    "#import theano.tensor as T\n",
    "\n",
    "#import lasagne\n",
    "#print(theano.generated_version.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LASAGNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# This is just some way of getting the MNIST dataset from an online location\n",
    "# and loading it into numpy arrays. It doesn't involve Lasagne at all.\n",
    "def load_dataset_medical():\n",
    "    def load_medical_images(filename):\n",
    "        with open(filename, 'rb') as f22:\n",
    "            data = pickle.load(f22)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_medical_labels(filename):\n",
    "        with open(filename, 'rb') as f22:\n",
    "            data = pickle.load(f22)\n",
    "            data = [int(row)  for row in data]\n",
    "        return np.asarray(data,dtype=int32)\n",
    "    \n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_medical_images('dataX.pkl')\n",
    "    y_train = load_medical_labels('dataY.pkl')\n",
    "    X_test =  load_medical_images('dataX.pkl')\n",
    "    y_test = load_medical_labels('dataY.pkl')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-140], X_train[-140:]\n",
    "    y_train, y_val = y_train[:-140], y_train[-140:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test    \n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn100(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 200, 200),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=.5),\n",
    "            num_units=51,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "cnn builded\n",
      "loss compiled \n",
      "train_fn compiled \n",
      "validation fn compiled \n",
      "compiled all functions \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset_medical()\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "#mlp\n",
    "#network = build_mlp(input_var)\n",
    "network = build_cnn100(input_var)\n",
    "print(\"cnn builded\")\n",
    "\n",
    "\n",
    "#custom_mlp\n",
    "#depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "#network = build_custom_mlp(input_var, int(depth), int(width),float(drop_in), float(drop_hid))\n",
    "\n",
    "#cnn\n",
    "#network = build_cnn(input_var)\n",
    "\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "test_loss = test_loss.mean()\n",
    "print(\"loss compiled \")\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates,allow_input_downcast=True)\n",
    "print(\"train_fn compiled \")\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc],allow_input_downcast=True)\n",
    "print(\"validation fn compiled \")\n",
    "\n",
    "print(\"compiled all functions \")\n",
    "\n",
    "# Finally, launch the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949, 1, 200, 200)\n",
      "(949,)\n",
      "(200, 200)\n",
      "(1089, 1, 200, 200)\n",
      "(1089,)\n",
      "28\n",
      "(949,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc517c91c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAEACAYAAABYh3hbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndK4Iztt5O286VwTn7bydN0n/H4el34NXMusuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc52c9cbdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print (X_train[0][0]).shape\n",
    "\n",
    "X2=[]\n",
    "with open('dataX.pkl', 'rb') as f:\n",
    "     data_newX = pickle.load(f)\n",
    "with open('dataY.pkl', 'rb') as f:\n",
    "     data_newY = pickle.load(f)\n",
    "#data_newY2 = [map(int, '99') for x in data_newY]  \n",
    "data_newY2 = [int(row)  for row in data_newY]\n",
    "print np.array(data_newX).shape       \n",
    "print np.array(data_newY).shape       \n",
    "print np.array(data_newY2[205])       \n",
    "print(y_train.shape)\n",
    "imshow(np.array(data_newX)[205][0],cmap=cm.Greys_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "epoch is 0\n",
      "Epoch 1 of 25 took 114.244s\n",
      "  training loss:\t\t3.292148\n",
      "  validation loss:\t\t2.663388\n",
      "  validation accuracy:\t\t22.50 %\n",
      "epoch is 1\n",
      "Epoch 2 of 25 took 109.674s\n",
      "  training loss:\t\t1.985209\n",
      "  validation loss:\t\t0.857703\n",
      "  validation accuracy:\t\t76.67 %\n",
      "epoch is 2\n",
      "Epoch 3 of 25 took 113.138s\n",
      "  training loss:\t\t0.732364\n",
      "  validation loss:\t\t0.439626\n",
      "  validation accuracy:\t\t90.00 %\n",
      "epoch is 3\n",
      "Epoch 4 of 25 took 111.158s\n",
      "  training loss:\t\t0.592054\n",
      "  validation loss:\t\t0.177721\n",
      "  validation accuracy:\t\t95.83 %\n",
      "epoch is 4\n",
      "Epoch 5 of 25 took 113.965s\n",
      "  training loss:\t\t0.377897\n",
      "  validation loss:\t\t0.140411\n",
      "  validation accuracy:\t\t96.67 %\n",
      "epoch is 5\n",
      "Epoch 6 of 25 took 109.762s\n",
      "  training loss:\t\t0.221469\n",
      "  validation loss:\t\t0.096384\n",
      "  validation accuracy:\t\t96.67 %\n",
      "epoch is 6\n",
      "Epoch 7 of 25 took 112.796s\n",
      "  training loss:\t\t0.173379\n",
      "  validation loss:\t\t0.057779\n",
      "  validation accuracy:\t\t97.50 %\n",
      "epoch is 7\n",
      "Epoch 8 of 25 took 111.082s\n",
      "  training loss:\t\t0.115479\n",
      "  validation loss:\t\t0.114149\n",
      "  validation accuracy:\t\t95.83 %\n",
      "epoch is 8\n",
      "Epoch 9 of 25 took 110.276s\n",
      "  training loss:\t\t0.082464\n",
      "  validation loss:\t\t0.054301\n",
      "  validation accuracy:\t\t97.50 %\n",
      "epoch is 9\n",
      "Epoch 10 of 25 took 110.596s\n",
      "  training loss:\t\t0.120339\n",
      "  validation loss:\t\t0.026885\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 10\n",
      "Epoch 11 of 25 took 110.354s\n",
      "  training loss:\t\t0.107072\n",
      "  validation loss:\t\t0.042112\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 11\n",
      "Epoch 12 of 25 took 110.573s\n",
      "  training loss:\t\t0.087358\n",
      "  validation loss:\t\t0.038760\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 12\n",
      "Epoch 13 of 25 took 110.627s\n",
      "  training loss:\t\t0.073738\n",
      "  validation loss:\t\t0.020184\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 13\n",
      "Epoch 14 of 25 took 110.688s\n",
      "  training loss:\t\t0.046752\n",
      "  validation loss:\t\t0.016926\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 14\n",
      "Epoch 15 of 25 took 111.275s\n",
      "  training loss:\t\t0.078093\n",
      "  validation loss:\t\t0.018387\n",
      "  validation accuracy:\t\t100.00 %\n",
      "epoch is 15\n",
      "Epoch 16 of 25 took 120.195s\n",
      "  training loss:\t\t0.063551\n",
      "  validation loss:\t\t0.023427\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 16\n",
      "Epoch 17 of 25 took 118.402s\n",
      "  training loss:\t\t0.054186\n",
      "  validation loss:\t\t0.019520\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 17\n",
      "Epoch 18 of 25 took 112.763s\n",
      "  training loss:\t\t0.025660\n",
      "  validation loss:\t\t0.024208\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 18\n",
      "Epoch 19 of 25 took 109.454s\n",
      "  training loss:\t\t0.060322\n",
      "  validation loss:\t\t0.009323\n",
      "  validation accuracy:\t\t100.00 %\n",
      "epoch is 19\n",
      "Epoch 20 of 25 took 112.502s\n",
      "  training loss:\t\t0.054072\n",
      "  validation loss:\t\t0.012694\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 20\n",
      "Epoch 21 of 25 took 114.784s\n",
      "  training loss:\t\t0.046052\n",
      "  validation loss:\t\t0.006157\n",
      "  validation accuracy:\t\t100.00 %\n",
      "epoch is 21\n",
      "Epoch 22 of 25 took 112.435s\n",
      "  training loss:\t\t0.049775\n",
      "  validation loss:\t\t0.031837\n",
      "  validation accuracy:\t\t98.33 %\n",
      "epoch is 22\n",
      "Epoch 23 of 25 took 119.148s\n",
      "  training loss:\t\t0.045458\n",
      "  validation loss:\t\t0.010586\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 23\n",
      "Epoch 24 of 25 took 111.350s\n",
      "  training loss:\t\t0.038890\n",
      "  validation loss:\t\t0.009511\n",
      "  validation accuracy:\t\t99.17 %\n",
      "epoch is 24\n",
      "Epoch 25 of 25 took 111.515s\n",
      "  training loss:\t\t0.037649\n",
      "  validation loss:\t\t0.014529\n",
      "  validation accuracy:\t\t99.17 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "num_epochs=25\n",
    "try:     \n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        print (\"epoch is %s\" % epoch)\n",
    "        for batch in iterate_minibatches(X_train, y_train, 40, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 40, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "   \n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"The train was interrupted in %d\" % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training, we compute and print the test error\n",
      "Final results:\n",
      "  test loss:\t\t\t0.011152\n",
      "  test accuracy:\t\t99.63 %\n"
     ]
    }
   ],
   "source": [
    " # After training, we compute and print the test error:\n",
    "print(\"After training, we compute and print the test error\")\n",
    "try:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, y_test, 20, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"The train was interrupted in %d\" % epoch)\n",
    "#save model\n",
    "savez('modelcnn_med1089.npz',*lasagne.layers.get_all_param_values(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33]\n",
      "Predicted 25 True 17\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADtCAYAAABTTfKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVLTkNAp9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc53ab84f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#getting prediction\n",
    "def plot_mnist_sample(sample):\n",
    "    imshow(sample[0],cmap=cm.Greys_r)\n",
    "    xticks([])\n",
    "    yticks([])\n",
    "\n",
    "    \n",
    "    \n",
    "predictions=T.argmax(test_prediction,axis=1)\n",
    "predict =theano.function (inputs=[input_var],\n",
    "                         outputs=predictions,allow_input_downcast=True)\n",
    "\n",
    "inputs, targets=iterate_minibatches(X_val, y_val,50, shuffle=True).next()\n",
    "preds=predict(inputs)\n",
    "bad_samples = where(preds!=targets)[0]\n",
    "if len(bad_samples)>0:\n",
    "    print (bad_samples)\n",
    "    error=random.choice(bad_samples)\n",
    "    plot_mnist_sample(X_val[error])\n",
    "    print(\"Predicted %d True %d\" % (preds [error],targets[error]))\n",
    "else:\n",
    "    print (\"No error predict found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_mnist_sample2(sample):\n",
    "    #print sample[0]\n",
    "    imshow(sample[0],cmap=cm.Greys_r)\n",
    "    xticks([])\n",
    "    yticks([])\n",
    "\n",
    "inputs, targets=iterate_minibatches(X_val, y_val, 500, shuffle=False).next()\n",
    "preds=predict(inputs)\n",
    "\n",
    "good_samples = where(preds==targets)[0]\n",
    "#print (good_samples)\n",
    "good_work=random.choice(good_samples)\n",
    "plot_mnist_sample2(X_val[good_work])\n",
    "print(\"Predicted %d True %d\" % (preds [good_work],targets[good_work]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Belif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import six.moves.cPickle as pickle\n",
    "def load_data2(dataset):\n",
    "    ''' Loads the dataset\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    "\n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    "\n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(__file__)[0],\n",
    "            \"..\",\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        \n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    "\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        from six.moves import urllib\n",
    "        origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        )\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('... loading data')\n",
    "\n",
    "    # Load the dataset\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        try:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        except:\n",
    "            train_set, valid_set, test_set = pickle.load(f)\n",
    "    # train_set, valid_set, test_set format: tuple(input, target)\n",
    "    # input is a numpy.ndarray of 2 dimensions (a matrix)\n",
    "    # where each row corresponds to an example. target is a\n",
    "    # numpy.ndarray of 1 dimension (vector) that has the same length as\n",
    "    # the number of rows in the input. It should give the target\n",
    "    # to the example with the same index in the input.\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,\n",
    "                 activation=T.tanh):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.dmatrix\n",
    "        :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: theano.Op or function\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        # end-snippet-1\n",
    "\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        \n",
    "        \n",
    "##########################################\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "##########################################\n",
    "\n",
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        # tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "\n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)\n",
    "##########################################\n",
    "# start-snippet-1\n",
    "class SdA(object):\n",
    "    \"\"\"Stacked denoising auto-encoder class (SdA)\n",
    "\n",
    "    A stacked denoising autoencoder model is obtained by stacking several\n",
    "    dAs. The hidden layer of the dA at layer `i` becomes the input of\n",
    "    the dA at layer `i+1`. The first layer dA gets as input the input of\n",
    "    the SdA, and the hidden layer of the last dA represents the output.\n",
    "    Note that after pretraining, the SdA is dealt with as a normal MLP,\n",
    "    the dAs are only used to initialize the weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        n_ins=784,\n",
    "        hidden_layers_sizes=[500, 500],\n",
    "        n_outs=10,\n",
    "        corruption_levels=[0.1, 0.1]\n",
    "    ):\n",
    "        \"\"\" This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the sdA\n",
    "\n",
    "        :type n_layers_sizes: list of ints\n",
    "        :param n_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "\n",
    "        :type corruption_levels: list of float\n",
    "        :param corruption_levels: amount of corruption to use for each\n",
    "                                  layer\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.dA_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
    "                                 # [int] labels\n",
    "        # end-snippet-1\n",
    "\n",
    "        # The SdA is an MLP, for which all weights of intermediate layers\n",
    "        # are shared with a different denoising autoencoders\n",
    "        # We will first construct the SdA as a deep multilayer perceptron,\n",
    "        # and when constructing each sigmoidal layer we also construct a\n",
    "        # denoising autoencoder that shares weights with that layer\n",
    "        # During pretraining we will train these autoencoders (which will\n",
    "        # lead to chainging the weights of the MLP as well)\n",
    "        # During finetunining we will finish training the SdA by doing\n",
    "        # stochastich gradient descent on the MLP\n",
    "\n",
    "        # start-snippet-2\n",
    "        for i in range(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden units of\n",
    "            # the layer below or the input size if we are on the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the hidden\n",
    "            # layer below or the input of the SdA if you are on the first\n",
    "            # layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            # its arguably a philosophical question...\n",
    "            # but we are going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the StackedDAA\n",
    "            # the visible biases in the dA are parameters of those\n",
    "            # dA, but not the SdA\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct a denoising autoencoder that shared weights with this\n",
    "            # layer\n",
    "            dA_layer = dA(numpy_rng=numpy_rng,\n",
    "                          theano_rng=theano_rng,\n",
    "                          input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i],\n",
    "                          W=sigmoid_layer.W,\n",
    "                          bhid=sigmoid_layer.b)\n",
    "            self.dA_layers.append(dA_layer)\n",
    "        # end-snippet-2\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs\n",
    "        )\n",
    "\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        # construct a function that implements one step of finetunining\n",
    "\n",
    "        # compute the cost for second phase of training,\n",
    "        # defined as the negative log likelihood\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size):\n",
    "        ''' Generates a list of functions, each of them implementing one\n",
    "        step in trainnig the dA corresponding to the layer with same index.\n",
    "        The function will require as input the minibatch index, and to train\n",
    "        a dA you just need to iterate, calling the corresponding function on\n",
    "        all minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared variable that contains all datapoints used\n",
    "                            for training the dA\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during training for any of\n",
    "                              the dA layers\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            cost, updates = dA.get_cost_updates(corruption_level,\n",
    "                                                learning_rate)\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[\n",
    "                    index,\n",
    "                    theano.In(corruption_level, value=0.2),\n",
    "                    theano.In(learning_rate, value=0.1)\n",
    "                ],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin: batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on\n",
    "        a batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                         the has to contain three pairs, `train`,\n",
    "                         `valid`, `test` in this order, where each pair\n",
    "                         is formed of two Theano variables, one for the\n",
    "                         datapoints, the other for the labels\n",
    "\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "        '''\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches //= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches //= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = [\n",
    "            (param, param - gparam * learning_rate)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='train'\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='test'\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            },\n",
    "            name='valid'\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def y_score():\n",
    "            return [y_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def pred_score():\n",
    "            return [pred_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def p_y_given_score():\n",
    "            return [p_y_given_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        \n",
    "        return train_fn, valid_score, test_score, pred_score, y_score, p_y_given_score\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "##########################################\n",
    "class RBM(object):\n",
    "    \"\"\"Restricted Boltzmann Machine (RBM)  \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RBM constructor. Defines the parameters of the model along with\n",
    "        basic operations for inferring hidden from visible (and vice-versa),\n",
    "        as well as for performing CD updates.\n",
    "\n",
    "        :param input: None for standalone RBMs or symbolic variable if RBM is\n",
    "        part of a larger graph.\n",
    "\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :param W: None for standalone RBMs or symbolic variable pointing to a\n",
    "        shared weight matrix in case RBM is part of a DBN network; in a DBN,\n",
    "        the weights are shared between RBMs and layers of a MLP\n",
    "\n",
    "        :param hbias: None for standalone RBMs or symbolic variable pointing\n",
    "        to a shared hidden units bias vector in case RBM is part of a\n",
    "        different network\n",
    "\n",
    "        :param vbias: None for standalone RBMs or a symbolic variable\n",
    "        pointing to a shared visible units bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        if numpy_rng is None:\n",
    "            # create a number generator\n",
    "            numpy_rng = numpy.random.RandomState(1234)\n",
    "\n",
    "        if theano_rng is None:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        if W is None:\n",
    "            # W is initialized with `initial_W` which is uniformely\n",
    "            # sampled from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible)) the output of uniform if\n",
    "            # converted using asarray to dtype theano.config.floatX so\n",
    "            # that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            # theano shared variables for weights and biases\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if hbias is None:\n",
    "            # create shared variable for hidden units bias\n",
    "            hbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='hbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if vbias is None:\n",
    "            # create shared variable for visible units bias\n",
    "            vbias = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='vbias',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input\n",
    "        if not input:\n",
    "            self.input = T.matrix('input')\n",
    "\n",
    "        self.W = W\n",
    "        self.hbias = hbias\n",
    "        self.vbias = vbias\n",
    "        self.theano_rng = theano_rng\n",
    "        # **** WARNING: It is not a good idea to put things in this list\n",
    "        # other than shared variables created in this function.\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "        # end-snippet-1\n",
    "\n",
    "    def free_energy(self, v_sample):\n",
    "        ''' Function to compute the free energy '''\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    def propup(self, vis):\n",
    "        '''This function propagates the visible units activation upwards to\n",
    "        the hidden units\n",
    "\n",
    "        Note that we return also the pre-sigmoid activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        ''' This function infers state of hidden units given visible units '''\n",
    "        # compute the activation of the hidden units given a sample of\n",
    "        # the visibles\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        # get a sample of the hiddens given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def propdown(self, hid):\n",
    "        '''This function propagates the hidden units activation downwards to\n",
    "        the visible units\n",
    "\n",
    "        Note that we return also the pre_sigmoid_activation of the\n",
    "        layer. As it will turn out later, due to how Theano deals with\n",
    "        optimizations, this symbolic variable will be needed to write\n",
    "        down a more stable computational graph (see details in the\n",
    "        reconstruction cost function)\n",
    "\n",
    "        '''\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        ''' This function infers state of visible units given hidden units '''\n",
    "        # compute the activation of the visible given the hidden sample\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        # get a sample of the visible given their activation\n",
    "        # Note that theano_rng.binomial returns a symbolic sample of dtype\n",
    "        # int64 by default. If we want to keep our computations in floatX\n",
    "        # for the GPU we need to specify to return the dtype floatX\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the hidden state'''\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        ''' This function implements one step of Gibbs sampling,\n",
    "            starting from the visible state'''\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # start-snippet-2\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        \"\"\"This functions implements one step of CD-k or PCD-k\n",
    "\n",
    "        :param lr: learning rate used to train the RBM\n",
    "\n",
    "        :param persistent: None for CD. For PCD, shared variable\n",
    "            containing old state of Gibbs chain. This must be a shared\n",
    "            variable of size (batch size, number of hidden units).\n",
    "\n",
    "        :param k: number of Gibbs steps to do in CD-k/PCD-k\n",
    "\n",
    "        Returns a proxy for the cost and the updates dictionary. The\n",
    "        dictionary contains the update rules for weights and biases but\n",
    "        also an update of the shared variable used to store the persistent\n",
    "        chain, if one is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # compute positive phase\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "\n",
    "        # decide how to initialize persistent chain:\n",
    "        # for CD, we use the newly generate hidden sample\n",
    "        # for PCD, we initialize from the old state of the chain\n",
    "        if persistent is None:\n",
    "            chain_start = ph_sample\n",
    "        else:\n",
    "            chain_start = persistent\n",
    "        # end-snippet-2\n",
    "        # perform actual negative phase\n",
    "        # in order to implement CD-k/PCD-k we need to scan over the\n",
    "        # function that implements one gibbs step k times.\n",
    "        # Read Theano tutorial on scan for more information :\n",
    "        # http://deeplearning.net/software/theano/library/scan.html\n",
    "        # the scan will return the entire Gibbs chain\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            # the None are place holders, saying that\n",
    "            # chain_start is the initial state corresponding to the\n",
    "            # 6th output\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k\n",
    "        )\n",
    "        # start-snippet-3\n",
    "        # determine gradients on RBM parameters\n",
    "        # note that we only need the sample at the end of the chain\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(\n",
    "            self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "        # end-snippet-3 start-snippet-4\n",
    "        # constructs the update dictionary\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(\n",
    "                lr,\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "        if persistent:\n",
    "            # Note that this works only if persistent is a shared variable\n",
    "            updates[persistent] = nh_samples[-1]\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(updates,\n",
    "                                                           pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "        # end-snippet-4\n",
    "\n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        \"\"\"Stochastic approximation to the pseudo-likelihood\"\"\"\n",
    "\n",
    "        # index of bit i in expression p(x_i | x_{\\i})\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # flip bit x_i of matrix xi and preserve all other bits x_{\\i}\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns\n",
    "        # the result to xi_flip, instead of working in place on xi.\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip -\n",
    "                                                            fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, updates, pre_sigmoid_nv):\n",
    "        \"\"\"Approximation to the reconstruction error\n",
    "\n",
    "        Note that this function requires the pre-sigmoid activation as\n",
    "        input.  To understand why this is so you need to understand a\n",
    "        bit about how Theano works. Whenever you compile a Theano\n",
    "        function, the computational graph that you pass as input gets\n",
    "        optimized for speed and stability.  This is done by changing\n",
    "        several parts of the subgraphs with others.  One such\n",
    "        optimization expresses terms of the form log(sigmoid(x)) in\n",
    "        terms of softplus.  We need this optimization for the\n",
    "        cross-entropy since sigmoid of numbers larger than 30. (or\n",
    "        even less then that) turn to 1. and numbers smaller than\n",
    "        -30. turn to 0 which in terms will force theano to compute\n",
    "        log(0) and therefore we will get either -inf or NaN as\n",
    "        cost. If the value is expressed in terms of softplus we do not\n",
    "        get this undesirable behaviour. This optimization usually\n",
    "        works fine, but here we have a special case. The sigmoid is\n",
    "        applied inside the scan op, while the log is\n",
    "        outside. Therefore Theano will only see log(scan(..)) instead\n",
    "        of log(sigmoid(..)) and will not apply the wanted\n",
    "        optimization. We can not go and replace the sigmoid in scan\n",
    "        with something else also, because this only needs to be done\n",
    "        on the last step. Therefore the easiest and more efficient way\n",
    "        is to get also the pre-sigmoid activation as an output of\n",
    "        scan, and apply both the log and sigmoid outside scan such\n",
    "        that Theano can catch and optimize the expression.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy\n",
    "\n",
    "##########################################\n",
    "class DBN(object):\n",
    "    \"\"\"Deep Belief Network\n",
    "\n",
    "    A deep belief network is obtained by stacking several RBMs on top of each\n",
    "    other. The hidden layer of the RBM at layer `i` becomes the input of the\n",
    "    RBM at layer `i+1`. The first layer RBM gets as input the input of the\n",
    "    network, and the hidden layer of the last RBM represents the output. When\n",
    "    used for classification, the DBN is treated as a MLP, by adding a logistic\n",
    "    regression layer on top.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
    "                 hidden_layers_sizes=[500, 500], n_outs=10):\n",
    "        \"\"\"This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the DBN\n",
    "\n",
    "        :type hidden_layers_sizes: list of ints\n",
    "        :param hidden_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector\n",
    "                                 # of [int] labels\n",
    "        # end-snippet-1\n",
    "        # The DBN is an MLP, for which all weights of intermediate\n",
    "        # layers are shared with a different RBM.  We will first\n",
    "        # construct the DBN as a deep multilayer perceptron, and when\n",
    "        # constructing each sigmoidal layer we also construct an RBM\n",
    "        # that shares weights with that layer. During pretraining we\n",
    "        # will train these RBMs (which will lead to chainging the\n",
    "        # weights of the MLP as well) During finetuning we will finish\n",
    "        # training the DBN by doing stochastic gradient descent on the\n",
    "        # MLP.\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden\n",
    "            # units of the layer below or the input size if we are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the\n",
    "            # hidden layer below or the input of the DBN if you are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "\n",
    "            # its arguably a philosophical question...  but we are\n",
    "            # going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the DBN. The visible\n",
    "            # biases in the RBM are parameters of those RBMs, but not\n",
    "            # of the DBN.\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct an RBM that shared weights with this layer\n",
    "            rbm_layer = RBM(numpy_rng=numpy_rng,\n",
    "                            theano_rng=theano_rng,\n",
    "                            input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layers_sizes[i],\n",
    "                            W=sigmoid_layer.W,\n",
    "                            hbias=sigmoid_layer.b)\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the\n",
    "        # negative log likelihood of the logistic regression (output) layer\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def predict_fn(self):\n",
    "        return theano.function(inputs=[self.x],outputs=[self.logLayer.y_pred])         \n",
    "    \n",
    "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
    "        '''Generates a list of functions, for performing one step of\n",
    "        gradient descent at a given layer. The function will require\n",
    "        as input the minibatch index, and to train an RBM you just\n",
    "        need to iterate, calling the corresponding function on all\n",
    "        minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared var. that contains all datapoints used\n",
    "                            for training the RBM\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "        :param k: number of Gibbs steps to do in CD-k / PCD-k\n",
    "\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "\n",
    "        # number of batches\n",
    "        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for rbm in self.rbm_layers:\n",
    "\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "            cost, updates = rbm.get_cost_updates(learning_rate,\n",
    "                                                 persistent=None, k=k)\n",
    "\n",
    "            # compile the theano function\n",
    "            #variable, name=None, value=None\n",
    "            #inputs=[index, theano.Param(learning_rate, default=0.1)],\n",
    "            fn = theano.function(\n",
    "                inputs=[index, theano.In(learning_rate, value=0.1)],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin:batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on a\n",
    "        batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                        the has to contain three pairs, `train`,\n",
    "                        `valid`, `test` in this order, where each pair\n",
    "                        is formed of two Theano variables, one for the\n",
    "                        datapoints, the other for the labels\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "\n",
    "        '''\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = []\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates.append((param, param - gparam * learning_rate))\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "        pred_score_i = theano.function(\n",
    "            [index],\n",
    "            self.logLayer.y_pred,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "        p_y_given_score_i = theano.function(\n",
    "            [index],\n",
    "            self.logLayer.p_y_given_x,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        y_score_i = theano.function(\n",
    "            [index],\n",
    "            self.y,\n",
    "            givens={\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def y_score():\n",
    "            return [y_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def pred_score():\n",
    "            return [pred_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def p_y_given_score():\n",
    "            return [p_y_given_score_i(i) for i in range(n_test_batches)]\n",
    "         \n",
    "        \n",
    "        \n",
    "        return train_fn, valid_score, test_score, pred_score, y_score, p_y_given_score\n",
    "\n",
    "############## igors Hybrid\n",
    "class RBM_DA(object):\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
    "                 hidden_layers_sizes=[[500,'rbm'], [500,'ae'],[300,'rbm'], [500,'ae']], n_outs=10):  #ig added\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.rbm_dA_layers = [] # ig added\n",
    "        self.sigmoid_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
    "        self.y = T.ivector('y')  # the labels are presented as 1D vector of [int] labels\n",
    "\n",
    "        i=0 \n",
    "        #for i in range(self.n_layers):\n",
    "        for ll in hidden_layers_sizes:\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                # ig input_size = hidden_layers_sizes[i - 1]\n",
    "                input_size = hidden_layers_sizes[i - 1][0]\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            #ig sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "            #                            input=layer_input,\n",
    "            #                            n_in=input_size,\n",
    "            #                            n_out=hidden_layers_sizes[i],\n",
    "            #                            activation=T.nnet.sigmoid)\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i][0],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "            \n",
    "            #ig\n",
    "            if ll[1]=='rbm':\n",
    "                trbm_dA_layer = RBM(numpy_rng=numpy_rng,\n",
    "                            theano_rng=theano_rng,\n",
    "                            input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layers_sizes[i][0],\n",
    "                            W=sigmoid_layer.W,\n",
    "                            hbias=sigmoid_layer.b)\n",
    "                self.rbm_dA_layers.append([trbm_dA_layer,ll[1]])\n",
    "            else:\n",
    "                trbm_dA_layer =  dA(numpy_rng=numpy_rng,\n",
    "                          theano_rng=theano_rng,\n",
    "                          input=layer_input,\n",
    "                          n_visible=input_size,\n",
    "                          n_hidden=hidden_layers_sizes[i][0],\n",
    "                          W=sigmoid_layer.W,\n",
    "                          bhid=sigmoid_layer.b)\n",
    "                self.rbm_dA_layers.append([trbm_dA_layer,ll[1]])\n",
    "\n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1][0],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "\n",
    "\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the\n",
    "        # negative log likelihood of the logistic regression (output) layer\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        corruption_level = T.scalar('corruption')  # % of corruption to use\n",
    "\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "\n",
    "\n",
    "\n",
    "        # number of batches\n",
    "        n_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        #for rbm in self.rbm_layers:\n",
    "        for rbm_dA in self.rbm_dA_layers:\n",
    "        #from sda for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "            if rbm_dA[1]=='rbm':\n",
    "                cost, updates = rbm_dA[0].get_cost_updates(learning_rate,persistent=None, k=k)\n",
    "                fn = theano.function(\n",
    "                     inputs=[index, theano.In(learning_rate, value=0.1)],\n",
    "                     outputs=cost,\n",
    "                     updates=updates,\n",
    "                     givens={self.x: train_set_x[batch_begin:batch_end]}\n",
    "                     )\n",
    "            else:\n",
    "                cost, updates = rbm_dA[0].get_cost_updates(corruption_level,learning_rate)\n",
    "                fn = theano.function(\n",
    "                     inputs=[index,theano.In(corruption_level, value=0.2),theano.In(learning_rate, value=0.1)],\n",
    "                     outputs=cost,\n",
    "                     updates=updates,\n",
    "                     givens={self.x: train_set_x[batch_begin:batch_end]}\n",
    "                     )\n",
    "\n",
    "\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "###########################3333\n",
    "\n",
    "###############################\n",
    "\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches /= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches /= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = []\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates.append((param, param - gparam * learning_rate))\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "        pred_score_i = theano.function(\n",
    "            [index],\n",
    "            self.logLayer.y_pred,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        p_y_given_score_i = theano.function(\n",
    "            [index],\n",
    "            self.logLayer.p_y_given_x,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        y_score_i = theano.function(\n",
    "            [index],\n",
    "            self.y,\n",
    "            givens={\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def y_score():\n",
    "            return [y_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def pred_score():\n",
    "            return [pred_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        def p_y_given_score():\n",
    "            return [p_y_given_score_i(i) for i in range(n_test_batches)]\n",
    "         \n",
    "        \n",
    "        \n",
    "        return train_fn, valid_score, test_score, pred_score, y_score, p_y_given_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = load_data2('mnist.pkl.gz')\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x, test_set_y = datasets[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets = load_data2('/opt/docker_share/mnist.pkl.gz')\n",
    "\n",
    "\n",
    "#pretraining_epochs=100\n",
    "k=1\n",
    "batch_size=10\n",
    "finetune_lr=0.1 \n",
    "training_epochs=100\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "\n",
    "# numpy random generator\n",
    "\n",
    "numpy_rng = numpy.random.RandomState(123)\n",
    "print '... building the model'\n",
    "    # construct the Deep Belief Network\n",
    "dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
    "          hidden_layers_sizes=[28*20, 28*16, 28*12],\n",
    "          n_outs=10)\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "print '... getting the pretraining functions'\n",
    "pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                            batch_size=batch_size,\n",
    "                                            k=k)\n",
    "########################\n",
    "# FINETUNING THE MODEL #\n",
    "########################\n",
    "# get the training, validation and testing function for the model\n",
    "\n",
    "#print datasets[1]\n",
    "\n",
    "print '... getting the finetuning functions'\n",
    "#train_fn, validate_model, test_model  = dbn.build_finetune_functions(datasets=datasets,batch_size=batch_size,learning_rate=finetune_lr)\n",
    "train_fn, validate_model, test_model, predict_fn, y_fn,p_y_given_score  = dbn.build_finetune_functions(\n",
    "    datasets=datasets,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=finetune_lr)\n",
    "\n",
    "print '... finetuning the model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretraining_epochs=10\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "pretrain_lr=0.01\n",
    "\n",
    "\n",
    "print '... pre-training the model'\n",
    "start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "for i in range(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "    for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "                if not(batch_index % 400):\n",
    "                    print 'Pre-training  epoch %d, index %i, %f, cost ' % (epoch, batch_index, numpy.mean(c))\n",
    "                    \n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "print >> sys.stderr, ('The pretraining code ran for %.2fm' % ((end_time - start_time) / 60.))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_epochs=10\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "patience_increase = 2.    # wait this much longer when a new best is found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "validation_frequency = min(n_train_batches, patience / 2)\n",
    "# go through this many\n",
    "# minibatches before checking the network\n",
    "# on the validation set; in this case we\n",
    "# check every epoch\n",
    "\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "#n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    \n",
    "    \n",
    "print 'start fine training the model %i' % n_train_batches\n",
    "while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        print 'epoch is %i' % epoch\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "\n",
    "print >> sys.stderr, ('The fine tuning code ran for %.2fm' % ((end_time - start_time)\n",
    "                                              / 60.))\n",
    "print 'end fine training the model'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "i=1 #0 ->2\n",
    "i=10 #3 ->4\n",
    "i=15 #7 ->0\n",
    "i=19 #9 ->3\n",
    "i=29 #7 ->2\n",
    "\n",
    "\n",
    "rast=train_set_x.get_value(borrow=True)[i]\n",
    "dbn.x=rast\n",
    "\n",
    "rast1=[]\n",
    "\n",
    "for ii in range(28):\n",
    "    rast1.append(rast[ii*28:(ii+1)*28])\n",
    "imshow(rast1,cmap=cm.Greys_r)\n",
    "\n",
    "\n",
    "\n",
    "p1=predict_fn()[i]\n",
    "y1=y_fn()[i]\n",
    "gx1=p_y_given_score()[i]\n",
    "print y1\n",
    "print p1\n",
    "#print gx1\n",
    "print min(y1==p1)\n",
    "#print (gx1)\n",
    "#print mean(p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len((dbn.rbm_layers[0].W.get_value())[1])\n",
    "print len(dbn.rbm_layers[0].params[1].get_value())\n",
    "\n",
    "#print(rast)\n",
    " \n",
    "x1=T.matrix('x1')\n",
    "#rast3=[]\n",
    "#for ii in range(len(rast)):\n",
    "#    rast3.append(rast[ii])\n",
    "#print rast3    \n",
    "#y = T.nnet.sigmoid(T.dot(dbn.rbm_layers[0].W, x) + dbn.rbm_layers[0].params[1])\n",
    "y1 = T.nnet.sigmoid(T.dot(x1,dbn.rbm_layers[0].W) + dbn.rbm_layers[0].params[1])\n",
    "y_0_res=theano.function(inputs=[x1],outputs=y1, allow_input_downcast=True)\n",
    "y_giv=y_0_res([rast])\n",
    "print len(y_giv[0])\n",
    "\n",
    "\n",
    "#print len(dbn.rbm_layers[0].b)\n",
    "print dbn.rbm_layers[1].params\n",
    "print dbn.logLayer.params\n",
    "print dbn.logLayer.y_pred\n",
    "from IPython.display import SVG\n",
    "SVG(theano.printing.pydotprint(dbn.logLayer.y_pred, return_image=True, format='svg'))\n",
    "theano.pprint(dbn.logLayer.y_pred)\n",
    "theano.printing.debugprint(dbn.logLayer.y_pred)\n",
    "dbn.x=rast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=10\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# STACKED AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets = load_data2('/opt/docker_share/mnist.pkl.gz')\n",
    "\n",
    "\n",
    "#pretraining_epochs=100\n",
    "k=1\n",
    "batch_size=10\n",
    "finetune_lr=0.1\n",
    "# compute number of minibatches for training, validation and testing\n",
    "\n",
    "# numpy random generator\n",
    "##########################\n",
    "# numpy random generator\n",
    "    # start-snippet-3\n",
    "    \n",
    "#numpy_rng = numpy.random.RandomState(123)\n",
    "numpy_rng = numpy.random.RandomState(89677)\n",
    "print '... building the model'\n",
    "    # construct the Deep Belief Network\n",
    "#dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,hidden_layers_sizes=[1000, 1000, 1000],n_outs=10)\n",
    "sda =  SdA(numpy_rng=numpy_rng, n_ins=28 * 28,hidden_layers_sizes=[28*20, 28*16, 28*12],n_outs=10)\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "print('... getting the pretraining functions')\n",
    "#pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,batch_size=batch_size,k=k)\n",
    "pretraining_fns = sda.pretraining_functions(train_set_x=train_set_x,batch_size=batch_size)\n",
    "\n",
    "print('... getting the finetuning functions')\n",
    "#train_fn, validate_model, test_model, predict_fn, y_fn,p_y_given_score  = dbn.build_finetune_functions(\n",
    "#    datasets=datasets,\n",
    "#    batch_size=batch_size,\n",
    "#    learning_rate=finetune_lr)\n",
    "train_fn, validate_model, test_model, predict_fn, y_fn,p_y_given_score = sda.build_finetune_functions(\n",
    "    datasets=datasets,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=finetune_lr  )\n",
    "print('done compile function')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretraining_epochs=10\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "#pretrain_lr=0.01\n",
    "pretrain_lr=0.001\n",
    "\n",
    "corruption_levels = [.1, .2, .3]\n",
    "\n",
    "\n",
    "\n",
    "print '... pre-training the model'\n",
    "start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "#for i in range(dbn.n_layers):\n",
    "for i in range(sda.n_layers):\n",
    "        # go through pretraining epochs\n",
    "    for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                #c.append(pretraining_fns[i](index=batch_index,lr=pretrain_lr))\n",
    "                c.append(pretraining_fns[i](index=batch_index,corruption=corruption_levels[i],lr=pretrain_lr))\n",
    "\n",
    "                \n",
    "                if not(batch_index % 400):\n",
    "                    print 'Pre-training  epoch %d, index %i, %f, cost ' % (epoch, batch_index, numpy.mean(c))\n",
    "                    \n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "print >> sys.stderr, ('The pretraining code ran for %.2fm' % ((end_time - start_time) / 60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_epochs=10\n",
    "\n",
    "# early-stopping parameters\n",
    "#patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "patience = 10 * n_train_batches  # look as this many examples regardless\n",
    "patience_increase = 2.    # wait this much longer when a new best is found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "validation_frequency = min(n_train_batches, patience / 2)\n",
    "# go through this many\n",
    "# minibatches before checking the network\n",
    "# on the validation set; in this case we\n",
    "# check every epoch\n",
    "\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "print 'start fine training the model %i' % n_train_batches\n",
    "while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        print 'epoch is %i' % epoch\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "\n",
    "print >> sys.stderr, ('The fine tuning code ran for %.2fm' % ((end_time - start_time)\n",
    "                                              / 60.))\n",
    "print 'end fine training the model'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM+AE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#datasets = load_data2('/opt/docker_share/mnist.pkl.gz')\n",
    "\n",
    "\n",
    "#pretraining_epochs=100\n",
    "k=1\n",
    "batch_size=10\n",
    "finetune_lr=0.1 \n",
    "training_epochs=100\n",
    "\n",
    "# compute number of minibatches for training, validation and testing\n",
    "\n",
    "# numpy random generator\n",
    "\n",
    "numpy_rng = numpy.random.RandomState(123)\n",
    "print '... building the model'\n",
    "    # construct the Deep Belief Network\n",
    "dbn = RBM_DA(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
    "          hidden_layers_sizes=[[28*20,'rbm'], [28*20,'ae'],[28*16,'rbm'], [28*16,'ae'],[28*12,'rbm'], [28*12,'ae'] ],\n",
    "          n_outs=10)\n",
    "\n",
    "\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "print '... getting the pretraining functions'\n",
    "pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                            batch_size=batch_size,\n",
    "                                            k=k)\n",
    "########################\n",
    "# FINETUNING THE MODEL #\n",
    "########################\n",
    "# get the training, validation and testing function for the model\n",
    "\n",
    "#print datasets[1]\n",
    "\n",
    "print '... getting the finetuning functions'\n",
    "#train_fn, validate_model, test_model  = dbn.build_finetune_functions(datasets=datasets,batch_size=batch_size,learning_rate=finetune_lr)\n",
    "train_fn, validate_model, test_model, predict_fn, y_fn,p_y_given_score  = dbn.build_finetune_functions(\n",
    "    datasets=datasets,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=finetune_lr)\n",
    "\n",
    "print '... finetuning the model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretraining_epochs=2\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "#pretrain_lr=0.01\n",
    "pretrain_lr=0.001\n",
    "\n",
    "corruption_levels = [.1,.1, .2,.2, .3, .3]\n",
    "\n",
    "\n",
    "\n",
    "print '... pre-training the model'\n",
    "start_time = timeit.default_timer()\n",
    "    ## Pre-train layer-wise\n",
    "\n",
    "i=0\n",
    "for rbm_dA in dbn.rbm_dA_layers:\n",
    "        #from sda for dA in self.dA_layers:\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "    print (\"Layer %d is %s\"%(i,rbm_dA[1]))\n",
    "    for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                if rbm_dA[1]=='rbm':\n",
    "                    c.append(pretraining_fns[i](index=batch_index,lr=pretrain_lr))\n",
    "                else:    \n",
    "                    c.append(pretraining_fns[i](index=batch_index,corruption=corruption_levels[i],lr=pretrain_lr))\n",
    "\n",
    "                \n",
    "                if not(batch_index % 400):\n",
    "                    print 'Pre-training  epoch %d, index %i, %f, cost ' % (epoch, batch_index, numpy.mean(c))\n",
    "                    \n",
    "            print 'Pre-training layer %i, epoch %d, cost ' % (i, epoch),\n",
    "            print numpy.mean(c)\n",
    "    i+=1\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "print >> sys.stderr, ('The pretraining code ran for %.2fm' % ((end_time - start_time) / 60.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_epochs=10\n",
    "\n",
    "# early-stopping parameters\n",
    "#patience = 4 * n_train_batches  # look as this many examples regardless\n",
    "patience = 10 * n_train_batches  # look as this many examples regardless\n",
    "patience_increase = 2.    # wait this much longer when a new best is found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "validation_frequency = min(n_train_batches, patience / 2)\n",
    "# go through this many\n",
    "# minibatches before checking the network\n",
    "# on the validation set; in this case we\n",
    "# check every epoch\n",
    "\n",
    "\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "print 'start fine training the model %i' % n_train_batches\n",
    "while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        print 'epoch is %i' % epoch\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%, '\n",
    "            'obtained at iteration %i, '\n",
    "            'with test performance %f %%'\n",
    "        ) % (best_validation_loss * 100., best_iter + 1, test_score * 100.)\n",
    "    )\n",
    "\n",
    "print >> sys.stderr, ('The fine tuning code ran for %.2fm' % ((end_time - start_time)\n",
    "                                              / 60.))\n",
    "print 'end fine training the model'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
